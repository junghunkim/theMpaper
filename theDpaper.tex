\documentclass[12pt]{article}%%contributed-IMS
\usepackage{amsthm,amsmath,amsfonts,amssymb,natbib}
\usepackage{graphicx}
\pagestyle{empty}

%\setlength{\textwidth}{7in}
%\setlength{\oddsidemargin}{-.25in}
%\setlength{\evensidemargin}{-.25in}


\newtheorem{lem}{Lemma}

\title{TBD}
\author{Nam H. Lee \& c}
\begin{document}
\maketitle

\section{Overview}
For each (undirected) pair $ij$ of $n$ vertices and each $t \in [0,1]$,
we let $N_{ij}(t)$ be the number of (undirected) communication events between vertex $i$ and vertex $j$, where each event is marked with one of the communication content topic classes $\{1,2\}$. 
Let, for each $t \in [0,1]$,
$$
\mathcal D(t) = \{ (\tau_\ell, ij_{\ell}, k_{\ell}) : \ell=1,\ldots, N(t) \},
$$ 
where $N(t) = \sum_{i<j} N_{ij}(t)$ and for simplicity, let 
$$
\mathcal D = \mathcal D(1).
$$ 

In this paper, we consider a problem of classifying the $n$ vertices into two groups after seeing $\mathcal D$, where the members of each class have the similar degrees of interests in topics from two content topic classes.  For this, we propose a classification algorithm derived from an EM-based paramter estimation procedure. 

To formulate our EM-based algorithm, we introduce a model for $\mathcal D$ through a data augmentation principle.  The hidden variables augmenting the data are the collection of random variables $\{\Lambda_{i,k}(t)\}$, where $\Lambda_{i,k}(t)$ is the degree of vertex $i$'s interest in topics from content topic class $k$ at time $t$. We assume that each $\Lambda_{i,k}(t)$ is a strictly positive random variable such that for some fixed constant $\lambda_i \in (0,\infty)$,
\begin{eqnarray}
\lambda_i = \Lambda_{i,1}(t) + \Lambda_{i,2}(t).
\end{eqnarray}
Our model, yielding the observation $\mathcal D$, is then to be a doubly stochastic marked Poisson process whose latent intensity process is specified by the $n\times 2$ random matrix
\begin{eqnarray}
\Lambda(t) = (\Lambda_{i,k}(t); i=1,\ldots,n, k=1,2).
\end{eqnarray}
For convenience, we denote the $i$-th row of $\Lambda(t)$ by $\Lambda_i(t)$. 

To demonstrate the peformance of our classification algorithm, we will also consider a so-called vertex nomination problem, in which the memembers of a particular class is already known and the objective is then to decide whether or not the class is to be expanded further and if to be expanded, which (single) vertex is to be included next. A solution to the vertex nomination problem is likely to be useful if some other information other than $\mathcal D$ is available.  On the other hand, we will also denomstrate the peformance of our classification algorithm when the only available information is $\mathcal D$ and the vertex nomination is then interpreted as a two-pass procedure.  

\subsection{Single-period model}
For each vertex $i$ and time $t\in[0,1]$, let 
\begin{eqnarray}
X_i(t) = (X_{i,1}(t), X_{i,2}(t)),
\end{eqnarray}
where
\begin{eqnarray}
X_{i,k}(t) = \frac{\Lambda_{i,k}(t)}{\Lambda_i(t)},
\end{eqnarray}
and note that each $X_i(t)$ is a probability vector. 
For each (undirected) pair $ij$, we assume that 
the process $N_{ij} = (N_{ij}(t):t\in[0,1])$ is   
a doubly stocastic Poisson process that whose intensity process is  
\begin{eqnarray}
\Lambda_{ij}(t) = \lambda_i \lambda_j (X_{i,1}(t)X_{j,1}(t) + X_{i,2}(t)X_{j,2}(t)).
\end{eqnarray}
A particularly useful chracterization is to describe each $N_{ij}$ as a process obtained 
by thinning a homogeneous Poisson process $M_{ij}$ whose constant intensity is $\lambda_i \lambda_j$.  
This affords an interpretation that each $t$ such that $M_{ij}(t) = M_{ij}(t-) +1$ 
is a potential communication opportunity between vertex $i$ and vertex $j$, 
and at each such $t$, depending on values of $X_i(t)$ and $X_j(t)$, 
the opportunity may or may not give rise to an actual communication event, i.e.\ $t$ such that $N_{ij}(t) = N_{ij}(t-) +1$. To be more precise, it is our assumption that given the filtration 
$$
\mathcal F_{ij}^\Lambda(t) = \sigma(\Lambda_i(s), \Lambda_j(t): s \le t), 
$$
if $M_{ij}(t) = M_{ij}(t-)+1$, then
$N_{ij}(t) = N_{ij}(t-) + 1$ with probability $\Lambda_{ij}(t)$ and the topic mark is of content class $k$ with probaiblity 
$$
\frac{X_{i,k}(t)X_{j,k}(t)}{X_{i,1}(t)X_{j,1}(t) + X_{i,2}(t)X_{j,2}(t)}
=
\frac{\Lambda_{i,k}(t)\Lambda_{j,k}(t)}{\Lambda_{i,1}(t)\Lambda_{j,1}(t) + \Lambda_{i,2}(t)\Lambda_{j,2}(t)}.
$$
For this, we begin by associating each vertex pair with a marked point process whose 
event occurrence and event marking are based on the latent state of the two vertice. 
More specifically, we augment the data $\mathcal M$ with 
the continuous-time (latent) state processes $\mathcal X = (X_1,\ldots, X_n)$ of $n$ vertice,
each taking values in a Polish space and with $n(n-1)/2$ homogeneous Poisson processes 
$\mathcal H= \{H_{ij} : 1 \le i < j \le n\}$.  

Here, each $X_i(t)$ represents the (latent) state of vertex $i$ at time $t$,
and each $H_{ij}(t)$ represents the total number of (potential) opportunities
that vertex $i$ and vertex $j$ could have exchanged a message (by time $t$).  
Then, the augmentating data $(\mathcal X, \mathcal H)$ and 
the augmented data $\mathcal M$ are associated by assuming that 
each $N_{ij}$ is obtained by  thinning each $H_{ij}$ using $X_i$ and $X_j$ 

We will also assume that each $X_i$ is a continuous-time strong Markov process 
such that its probability density with respect to the standard Brownian motion
exists.  
Now, fix a non-negative bounded Borel measurable function $\Lambda$ taking values in $\mathbb R_+$ such that $\Lambda(x,y) = \Lambda(y,x)$, and let
\begin{eqnarray}
\lambda = \sup_{x,y} \Lambda(x,y).
\end{eqnarray}
Then, we also assume that each $N_{ij}$ is a doubly stochastic Poisson process whose intensity at $t$  is 
$\Lambda(X_i(t),X_j(t))$.  
In particular, (conditioning on $X_i$ and $X_j$), $N_{ij}$ is obtained from $H_{ij}$ by independently deleting each occurrence (of potential messaging opportunity) in $H_{ij}$ with probability equaling the value of $1-\Lambda_{ij}(\cdot)/\|\Lambda_{ij}\|_\infty$ at the occurrence time of the event (messaging opportunity). 
Then, it follows that the likelihood of observing $\mathcal M$ is 
\begin{eqnarray}
f(\mathcal M(ij) ;X_i, X_j, \Psi) =  \Lambda_{ij}(\tau_1(ij)) \cdots \Lambda_{ij}(\tau_N(ij)) \exp\left(-\int_0^1 \Lambda_{ij}(s) ds\right),
\end{eqnarray}
where $\Psi$ is the parameter set specifying the entire model and 
\begin{eqnarray}
\Lambda_{ij}(s) = \Lambda(X_i(s),X_j(s)).
\end{eqnarray}  

\subsection{Random dot product \& Jacobi processes}
In the rest of this paper, we consider the model when $\Lambda$ is a ``dot-product'' function
and each $X_i$ is a Jacobi process.  That is, we assume that each $X_i$ is a diffusion 
process taking values in $[0,1]$ and satisfies (in the weak sense)
the following stochastic differential equation:
\begin{eqnarray}
dX_i(t) = \beta_i(X_i(t) - \gamma_i) dt + \sigma \sqrt{X_i(t) (1-X_i(t))} dW_i(t), 
\end{eqnarray}
where each $W_i(t)$ is a standard Brownian motion (starting at zero).  We also assume that the random elements $X_1(0),\ldots, X_n(0)$ and $W_1,\ldots, W_n$ are independent from each other.  Next, for each $x$ and $x^\prime \in [0,1]$, we let 
\begin{eqnarray}
\Lambda(x,y) = \lambda (x x^\prime + (1-x) (1-x^\prime)).
\end{eqnarray}
For future reference, let
\begin{eqnarray}
\overline{\Lambda}(x,y) = x x^\prime + (1-x) (1-x^\prime),
\end{eqnarray}
and also
\begin{eqnarray}
\psi_i = (\beta_i,\gamma_i,\sigma_i). 
\end{eqnarray}
Note that 
\begin{eqnarray}
\Psi = (\psi_1,\ldots, \psi_n,\lambda).
\end{eqnarray}

Our main result of this paper concerns an algorithm for correctly assigning each vertex into one of two classes, where witnin each group, all the members  
have the same parameter i.e.\ if vertex $i$ and vertex $j$ are in the same
class, then $\psi_i = \psi_j$.  The rest of this paper is organized as follows.
In Section 2, we develop a (Monte-Carlo) EM algorithm for estimating 
$\Psi$, where formally, no restriction on $\Psi$
other than requiring $\beta_i < 0$, $\gamma_i \in (0,1)$ and $\sigma_i > 0$ is imposed. 
In Section 3, using the EM algorithm from Section 2, we formulate an interative 
procedure for partitioning $n$ vertices into two classes, where in each iteration, the new partitioning is obtained by nominating one vertex to be switched from its current class to the opposite class (i.e. if the nominted vertex is of class $1$, then its new class will be class $2$).  In Section 4, we illustrate our methods for special cases.  In particular, for the spacial case, some convergence properties of the EM algorithm are illustrated and also, classification performance of the iterative nomination partitioning algorithm are demonstrated.  

\section{Greedy algorithm for vertex nomination}
In this section, we provide a greedy algorithm for arriving at a partition 
of the vertex set which is likely to be the true partition of 
the vertex set that  have produced the data $\mathcal M$.

For the initial step $q=1$, suppose that we have 
an arbitrary partition $(C_1(1), C_2(1))$ of $[n]$. 
Let $\Psi(1)$ be the output of our EM algorithm, where we assume that within each group,
all vertice have the same paramter.  Then, for each pair $i, j \in C_k(1)$, $\Psi_i(q) = \Psi_j(q)$.  
Let
\begin{eqnarray}
Q(1) : = \mathbf E_{\Psi(1)}\left[\log(f_{[0,T]}(\mathcal M, X;\Psi(1)))\right].
\end{eqnarray}

Next, for the $(q+1)$-st iteration, suppose that we have a partition $(C_1(q), C_2(q))$ from the $q$-th iteration. If $\mathcal C_2(q) = \varnothing$, then our partitioning algorithm terminates, yielding the $q$-th partition $(C_1(q), C_2(q))$ as the final output.  So, we assume that $C_2(q)\neq \varnothing$.  Then, for some $\ell > 0$ and $m > 0$ with $\ell + m = n$, we have 
\begin{eqnarray}
\mathcal C_1(q) = \{u_1,\ldots, u_\ell\},\\
\mathcal C_2(q) = \{v_1,\ldots, v_m\}.
\end{eqnarray} 
For each $i \in C_2(q)$, let 
\begin{eqnarray}
C_1^{i}(q) = C_1(q) \cup \{i\}, \\
C_2^{i}(q) = C_2(q) \setminus \{i\}.
\end{eqnarray}
Now, for each $i$, let $\Psi^{i}(q)$ be the EM estimate of $\Psi$ with the partition $C_1^{i}(q), C_2^{i}(q)$, and let 
\begin{eqnarray}
Q^{i}(q) = \mathbf E_{\Psi^{i}(q)}\left[\log(f_{[0,T]}(\mathcal M,X;\Psi^{i}(q)))\right].
\end{eqnarray}
If for all $i \in C_2(q)$, $Q^{i}(q) \le Q(q)$, then the algorithm terminates, yielding 
$(C_1(q), C_2(q))$ as the final partitioning.  Otherwise, we set 
\begin{eqnarray}
i^* = \arg\max_{i \in C_2(q)} Q^{i}(q),
\end{eqnarray}
and let 
\begin{eqnarray}
&C_1(q+1) = C_1(q) \cup \{i^*\},\\
&C_2(q+1) = C_2(q) \setminus \{i^*\}.
\end{eqnarray}

\section{Monte-Carlo EM algorithm}
In the last section, we assume the computability (and the existence) 
of the exact MLE of $\Psi$ from the data $\mathcal M$.   
Because no analytic formula (in general) for the marginal density 
of $\mathcal M$ is avaible,
we instead turn to an EM algorithm, in which each exact MLE is replaced 
by a sufficiently close estimate of the exact MLE.
Our formulation of the EM algorithm requires 
three main steps.  
At first, we deriving (analytically) 
the density of the random element $(\mathcal M, X)$.  
Next, we devise a conditional simulation algorithm for the $E$ step.  
Finally, we presents a closed-form solution for the $M$ step.
%\includegraphics[scale=0.75]{myOBJplot.pdf}

\subsection{Full likelihood function}
Let
\begin{enumerate}
\item[(i)] $M_{ij}$ is the (total) number of messages between vertex $i$ and vertex $j$,
\item[(2)] $\tau_{ij}(\ell)$ is the time at which occurred the $\ell$-th communication event 
between vertex $i$ and vertex $j$.
\end{enumerate}
The (full) likelihood function to be used in the expectation stage of our EM algorithm takes the following form:
\begin{eqnarray}
\prod_{i<j} f_{ij}(\mathcal M(ij);X_i,X_j,\Psi) \prod_{i=1}^n f_{[0,T],i}(X_i;\Psi),
\end{eqnarray}
where 
\begin{enumerate}
\item[(1)] $f_{[0,T],i}(\cdot;\Psi)$ is the likelihood function of the $i$-th vertex 
process' sample path (for the interval $[0,T]$),
\item[(2)] $f_{ij}(\cdot;X_i,X_j,\Psi)$ is the likelidhood of 
observing message at time 
$$
\tau_{ij}(1),\ldots, \tau_{ij}(M_{ij}),
$$
repsectively, about topic 
$$
\kappa_{ij}(1),\ldots, \kappa_{ij}(M_{ij}).
$$
\end{enumerate}

First, conditioning on the path of $X_i$ and $X_j$, and given the value of $\lambda$,   
the value of $f_{ij}(\cdot;X_i,X_j,\Psi)$ can be seen to be
\begin{eqnarray}\label{formula2010.12.19.12.31.pm}
\prod_{i<j} \prod_{\ell=1}^{M_{ij}} \Lambda_{\kappa_{ij}(\ell)}(X_i(\tau_{ij}(\ell)),X_j(\tau_{ij}(\ell)) \exp\left(-\int_0^T \Lambda(X_i(s),X_j(s)) ds\right).
\end{eqnarray}
Next, given the value of $\Psi$, the likelihood of the sample path of $X_i$ can be 
seen to satisfy:
\begin{eqnarray}\label{formula2010.12.19.01.07.pm}
\log(f_{[0,T],i}(X_i)) 
=
a(h(X_T)) - a(h(X_0)) 
-\frac{1}{2} \int_0^T \mu^2(h(X_t)) +\mu^\prime(h(X_t)) dt,
\end{eqnarray}
where 
\begin{eqnarray}
&h(x) = \frac{1}{\sigma} \arcsin(2x - 1),\\
&\mu(y) = \left(\frac{\beta}{\sigma} + \frac{\sigma}{2}\right) \tan(\sigma y)
+\frac{\beta(1-2\gamma)}{\sigma} \frac{1}{\cos(\sigma y)},\\
&a(y) = - \left(\frac{\beta}{\sigma^2}+\frac{1}{2}\right) \log(\cos(\sigma y))
+ \frac{\beta (1-2\gamma)}{2\sigma^2} \log\left(\frac{1+\sin(\sigma y)}{1-\sin(\sigma y)}\right),\\
&\mu^\prime(y) = 
(\beta+ \sigma^2/2) \frac{1}{\cos^2(\sigma y)} + \beta (1-2\gamma) \frac{\sin(\sigma y)}{\cos^2(\sigma y)}. 
\end{eqnarray}
Without going into all the details, we will briefly outline the elements of 
deriving these formulae displayed above.  


To see the formula in (\ref{formula2010.12.19.12.31.pm}), first recall the (general) fact that 
conditioning a intensity function $r(t)$ of an inhomogeneous Poisson process,
the likelihood for observing events at times $t_1,\ldots, t_M$ during interval $[0,T]$
is given by
\begin{eqnarray}
r(t_1)\cdots r(t_M) \exp\left(-\int_0^{T} r(s) ds\right),
\end{eqnarray}
and the fact for our (general) model that 
given that there were a communication event between vertex $i$ and vertex $j$ at time $t$,
the chance that the event is about topic $1$ is 
\begin{eqnarray}
\frac{X_i(t)X_j(t)}{\Lambda(X_i(t),X_j(t))} 
\end{eqnarray}
and the chance that event is about topic $2$ is 
\begin{eqnarray}
1-\frac{X_i(t)X_j(t)}{\Lambda(X_i(t),X_j(t))} 
=
\frac{(1-X_i(t))(1-X_j(t))}{\Lambda(X_i(t),X_j(t))}.
\end{eqnarray}
Combining these facts, one sees (\ref{formula2010.12.19.12.31.pm}). 

Next, let $\mathcal C$ be the set of all functions from $[0,T]$ to $\mathbb R$, and 
we equip the space $C$ with the usual $\sigma$-algebra generated by the cylinder sets. 
In (\ref{formula2010.12.19.01.07.pm}), 
each $f_{[0,T],i}$ is the Radon-Nykodim derivative of the probability measure induced by $X_i$ 
on $\mathcal C$ against the measure induced on $\mathcal C$ by the standard Brownian motion. 
To get the density $f_{[0,T],i}$, we follow the standard technique in which 
first, the ornigal diffusion is transformed to another diffusion whose
diffusion term is not state-dependent and, then apply Girsanov's theorem to obtain
the likelihood ratio formula, which is $f_{[0,T],i}$ above.  More details for these calculations 
will be presented in a later section, but also, a reader might find Michael Sorenson's 
paper useful. 

\subsection{Conditional sampling algorithms}
In the E step, one needs to know how to compute, for each $\Psi$,
\begin{eqnarray}
Q_\ell(\Psi) = \mathbf E_{\Psi_\ell,\mathcal M}\left[ \log f_{[0,T]}(\mathcal M, X^{(\ell)};\Psi)\right],
\end{eqnarray}
where the law of $X^{(\ell)}$ is the law of $X$ conditioning on the data $\mathcal M$ and the paramter $\Psi_\ell$.  The general closed-form expression of $Q(\Psi)$ is difficult to obtain, and we develop a way to obtain a close approximation $\widehat{Q}_\ell$ of $Q$.  To do this, we use a Monte Carlo estimate based on the rejection technique.  First, our simulation of each $X_i$ (uncondtionally) is achieved through discriteziation.  For example, we can fix a small $\Delta t > 0$ and 
\begin{eqnarray}
X_i(t+\Delta t) = X_i(t) + \beta_i(X_i(t) - \gamma_i) \Delta t + \sigma_i \sqrt{X_i(t) - X_i^2(t)} Z \Delta t,
\end{eqnarray}
where $Z$ is a standard normal random variable.  
Next, note that 
\begin{eqnarray}
f_{[0,T]}(X\left|\mathcal M\right.;\Psi) 
&\propto& f_{[0,T]}(X,\mathcal M;\Psi) \\
&=& f_{[0,T]}(\mathcal M\left|X,\Psi\right.) f_{[0,T]}(X\left|\Psi\right.)\\
&=& \prod_{i<j} f_{ij}(\mathcal M(ij);X_i,X_j,\Psi) \prod_{i=1}^n f_{[0,T],i}(X_i;\Psi),\\
&\le & \lambda^{\sum_{ij}M_{ij}} \prod_{i=1}^n f_{[0,T],i}(X_i;\Psi).
\end{eqnarray}
Therefore, (in theory), to simulate conitionally, it is enough to simulate 
$X$ unconditionally as a proposal, and accept the proposed sample path
with probability:
\begin{eqnarray}
\prod_{i<j} \left(
\exp\left(-\int_{0}^{T} \Lambda(X_i(s),X_j(s)) ds\right)
\prod_{\ell=1}^{M_{ij}}\overline{\Lambda}_{\kappa_{ij}(\ell)}(X_i(\tau_{ij}(\ell)),X_j(\tau_{ij}(\ell)))
\right)
. 
\end{eqnarray}

On the other hand, the rejection probability (as the algorithm is presented now) is not trivial and as $\sum_{i<j} M_{ij}$ gets larger, the second factor is likely to be a very small number.  To remedy this, we use a sequential rejection sampling prodedure.  First, we partition the interval $[0,T]$ into many pieces, say, with grid $0,t_1,\ldots, t_L$, so that each interval $[t_{\ell},t_{\ell+1})$ contains at most one messaging event (of any kind). Then, progressing from the left end point to the right end point recursively, starting from the $X(t_{\ell})$ obtained from the preceding interval simulation, simulate unconditionally.  Then, accept and append the proposed sample path segment with the probability:
\begin{eqnarray}
\exp\left(-\int_{t_{\ell}}^{t_{\ell+1}} \Lambda(X_i(s),X_j(s)) ds\right).
\end{eqnarray}
if the new interval $[t_{\ell},t_{\ell+1})$ has no messaging event but otherwise, accept and append the proposed sample path segment with the probability:
\begin{eqnarray}
\exp\left(-\int_{t_{\ell}}^{t_{\ell+1}} \overline{\Lambda}(X_i(s),X_j(s)) ds\right).
\end{eqnarray}

In our experiment, this sequential simulation technique significantly improve the efficiency of algorithm but it is important to note that the size of each $t_{i+1} - t_i$ is chosen with care since making it too small creates another rare event type issue, namely, always accepting the proposed sample path segment when ocassionally rejected.   
 
%\includegraphics[scale=0.75]{mydataplot.pdf}

\subsection{????}
In this section, we prove that our EM procedure is well-posed under
a mild condition.  In other words, for each iteration of the EM algorithm,
a solution to the optimization problem in the M step exists and is unique.
To do this, we begin by providing the details 
of deriving the full likelihood function. This requires a popular techinque
in dealing with a Ito (diffusion) process, where after a one-to-one trasnformation, the process becomes another Ito process whose diffusion term is simply a standard Brownian motion.

To begin, for each vertex $i$, let
\begin{eqnarray}
h_i(x) = \frac{1}{\sigma_i}\arcsin(2x-1) = \int \frac{1}{\sigma_i\sqrt{x-x^2}}dx.
\end{eqnarray}
Note that each $h_i$ is one-to-one (cf.\ Lamperti transformation) and let
\begin{eqnarray}
H_i(t) = \frac{1}{\sigma_i}\arcsin(2X_i(t)-1).
\end{eqnarray}
Then, an application of Ito's formula yields:
\begin{eqnarray}
dH_i(t) = \mu_i(H_i(t);\psi) dt + dW_i(t),
\end{eqnarray}
where
\begin{eqnarray*}
&\ & \mu_i(H_i(t);\psi) \\
%&=& \frac{b_i(X_i(t);\psi)}{\sigma_i(X_i(t);\psi)} - \frac{1}{2} \sigma_i^\prime(X_i(t);\psi)\\
&=& \frac{\beta_i}{\sigma_i} \frac{X_i(t)-\gamma_i}{\sqrt{X_i(t)-X_i^2(t)}}
-  \frac{1}{2} \frac{\sigma_i/2 (1-2X_i(t))}{\sqrt{X_i(t) - X_i^2(t)}}.%\\
%&=&
%\left(\frac{\beta_i}{\sigma_i} + \sigma_i/2\right)\frac{X_i(t)}{\sqrt{X_i(t) - X_i^2(t)}}
%-
%\left(\frac{\beta_i \gamma_i}{\sigma_i} + \frac{\sigma_i}{4}\right)
%\frac{1}{\sqrt{X_i(t) - X_i^2(t)}}\\
%&=&
%\left(\frac{\beta_i}{\sigma_i} + \sigma_i/2\right)\frac{2X_i(t)-1 +1}{2\sqrt{X_i(t) - X_i^2(t)}}
%-
%\left(\frac{\beta_i \gamma_i}{\sigma_i} + \frac{\sigma_i}{4}\right)
%\frac{1}{\sqrt{X_i(t) - X_i^2(t)}}\\
%&=&
%\left(\frac{\beta_i}{\sigma_i} + {\sigma_i}/2\right)
%\left( \tan(\sigma_i H_i(t)) + \sec(\sigma_i H_i(t))\right)
%-
%\left(\frac{2\beta_i \gamma_i}{\sigma_i} + {\sigma_i}/2\right)
%\sec(\sigma_i H_i(t)) \\
%&=&
%\left(\frac{\beta_i}{\sigma_i} + {\sigma_i}/2\right)
%\tan(\sigma_i H_i(t))
%-
%\frac{\beta_i}{\sigma_i} (1-2\gamma_i)
%\sec(\sigma_i H_i(t)).
\end{eqnarray*}
Next, for each $\psi \in \Psi$, let $P_\psi$ be the probability distribution induced by $H = (H_1,\ldots, H_n)$ on $(C\times\cdots\times C,\ldots, \mathcal C\times\cdots\times \mathcal C)$, and note that by the Girsanov theorem and Ito's formula,  
one can obtain the following Radon-Nykodym density:
\begin{eqnarray*}
\frac{dP_{\psi}}{dQ}(H)
&= &
\prod_{i=1}^n \exp\left(
\int_0^T \mu_i(H_i(t);\psi)dH_i(t)
-
\frac{1}{2} \int_0^T \mu_i^2(H_i(t);\psi)dt\right)\\
&= &
\prod_{i=1}^n \exp\left(
a_i(\sigma H_i(T);\psi) - a_i(\sigma H_i(0);\psi)
-\frac{1}{2} \int_0^T \mu_i^2(H_i(t);\psi) + \mu_i^\prime(H_i(t);\psi) dt\right),
\end{eqnarray*}
where
\begin{eqnarray*}
a_i(x;\psi) = \int \mu_i(x;\psi) dx.
\end{eqnarray*}
Now, it can be shown that:
\begin{eqnarray*}
&\ &a_i(\sigma H_i(t);\psi)  \\
%&= &
%\left(\frac{\beta_i}{\sigma_i} + \sigma_i/2\right)
%\frac{1}{\sigma_i}\log( \sec(\sigma_i H_i(t)) )
%+
%\frac{-\beta_i}{\sigma_i} (1-2\gamma_i) \frac{1}{\sigma_i}
%(\log( \sec(\sigma_i H_i(t)) + \tan(\sigma_i H_i(t))))\\
%&= &
%\left(\frac{\beta_i}{\sigma_i^2} + 1/2\right)
%\log( \sec(\sigma_i H_i(t)) )
%+
%\frac{-\beta_i}{\sigma_i^2} (1-2\gamma_i)
%\left(\log( \sec(\sigma_i H_i(t)) + \tan(\sigma_i H_i(t)))\right)\\
&= &
\left(\frac{\beta_i}{\sigma_i^2} + 1/2\right)
\log\left( \frac{1}{2\sqrt{X_i(t) - X_i^2(t)}} \right)
+
\frac{-\beta_i}{\sigma_i^2} (1-2\gamma_i)
\log\left( \frac{X_i(t)}{\sqrt{X_i(t) - X_i^2(t)}} \right),
\end{eqnarray*}
and
\begin{eqnarray*}
&\ & \mu_i^\prime(\sigma_i H_i(t);\psi) \\
%&= & \left(\frac{\beta_i}{\sigma_i} + {\sigma_i}/2\right)
%\sigma_i
%\sec^2(\sigma_i H_i(t))
%+
%\frac{-\beta_i}{\sigma_i} (1-2\gamma_i)
%\sigma_i \sec(\sigma_i H_i(t)) \tan(\sigma_i H_i(t))\\
%&= & \left(\beta_i + {\sigma_i^2}/2\right)
%\sec^2(\sigma_i H_i(t))
%+
%\frac{-\beta_i}{\sigma_i} (1-2\gamma_i)
%\sec(\sigma_i H_i(t)) \tan(\sigma_i H_i(t))\\
&= &
\left(\beta_i + {\sigma_i^2}/2\right)
\frac{1}{\left(2\sqrt{X_i(t) - X_i^2(t)}\right)^2}
+
\frac{-\beta_i}{\sigma_i} (1-2\gamma_i)
\frac{2X_i(t)-1}{\left(2\sqrt{X_i(t) - X_i^2(t)}\right)^2}.
\end{eqnarray*}
Then, the likelihood $f_{[0,T]}(\mathcal M,H;\Psi)$ of $(\mathcal M,H)$ for the interval $[0,T]$ equals
\begin{eqnarray*}
&\ & \prod_{i<j} \left( \exp\left(-\int_0^{T} \Lambda_{ij}(s)ds\right) \prod_{\ell=1}^{M_{ij}}\Lambda_{ij,\kappa_\ell(ij)}(\tau_\ell(ij))\right)\\
&\ & \ \ \cdot
\prod_{i=1}^n \exp\left(
\left. a_i(H_i(t);\Psi) \right|_{t=0}^{t=T}-\frac{1}{2} \int_0^T \mu_i^2(H_i(t);\Psi) + \mu_i^\prime(H_i(t),\Psi) dt\right).
\end{eqnarray*}

In the $\ell$-th iteration of our EM algorithm, we have the current estimate $\Psi_\ell$.  For each $\Psi$, let
\begin{eqnarray}
Q_{\ell}(\Psi) = \mathbf E_{\ell}
\left[
\log f_{[0,T]}(X,\mathcal M;\Psi)
\right],
\end{eqnarray}
where the expectation $\mathbf E_\ell$ is taken conditionally on $\mathcal M$ with the underlying parameter $\Psi_\ell$. Then, we obtain our next (improved) estimate $\Psi_{\ell+1}$ by finding the location of the maximum of the function $Q_{\ell}$.

We now find an analytic formula for $Q_\ell$ explicit in terms of parameter $\Psi$. 
To do this, we first let:
\begin{eqnarray*}
Q_{\ell,0}(\Psi) 
&=& -\lambda_\ell \left( \sum_{i<j} \int_0^T \langle \overline{\Lambda}_{ij}(s)\rangle ds\right)\\
& & + \left(\sum_{i<j} \sum_{m=1}^{M_{ij}} \log(\lambda_\ell) \right)\\
& & + \left( \sum_{ij} \sum_{m=1}^{M_{ij}} \langle \log \overline \Lambda_{ij,\kappa_{m}(ij)}(\tau_m(ij))\rangle \right)
\end{eqnarray*}

\begin{eqnarray*}
Q_{\ell,1}(\Psi) 
&=& \sum_{i=1}^n \left(\frac{\beta_i}{\sigma_i^2} + \frac{1}{2}\right) \langle A_i(T) - A_i(0) \rangle_\ell \\
&\ & +   \sum_{i=1}^n \left(-\frac{\beta_i}{\sigma_i^2} - 2\gamma_i\right)\langle B_i(T) - B_i(0) \rangle_\ell. 
\end{eqnarray*}


\begin{eqnarray*}
Q_{\ell,2}(\Psi) &=& \sum_{i=1}^n \left(\frac{\beta_i}{\sigma_i} + \frac{\sigma_i}{2}\right)^2 \int_0^T \langle E_i(t)\rangle_\ell dt  \\
&\ & + \left( \frac{\beta_i \gamma_i}{\sigma_i} + \frac{\sigma_i}{4}\right)^2
\int_0^T \langle F_i(t)\rangle_\ell dt  \\
&\ & + 
\left(\frac{\beta_i}{\sigma_i} + \frac{\sigma_i}{2}\right)
\left(-\frac{\beta \gamma_i}{\sigma_i} - \frac{\sigma_i}{4}\right)
\int_0^T \langle G_i(t) \rangle_\ell dt.
\end{eqnarray*}

\begin{eqnarray*}
Q_{\ell,3}(\Psi) &=& \sum_{i=1}^n \left( \beta_i + \frac{\sigma_i^2}{2}\right) \int_0^T \langle C_i(t) \rangle_\ell dt\\
&\ & + \sum_{i=1}^n \left(-\frac{\beta_i}{\sigma_i} (1-2\gamma_i) \right) \int_0^T \langle D_i(t) \rangle_\ell dt. 
\end{eqnarray*}

Note that 
\begin{eqnarray}
Q_\ell(\Psi) = 
Q_{\ell,0}(\Psi)+
Q_{\ell,1}(\Psi)+
Q_{\ell,2}(\Psi)+
Q_{\ell,3}(\Psi).
\end{eqnarray}
and each term $Q_{\ell,k}$ is a polynomial whose degree is at most two in each letter in $\Psi$.
In particular, the partial derivate of $Q_\ell$ with respect to each letter in $\Psi$ yields a linear equation and as a necessary condition for a unique optimizer existing in the interior of the parameter space for $\Psi$ is that existence of a unique solution to a system of linear equations. Also, recall that we require for each vertex $i$ that 
\begin{eqnarray}
&\beta_i \in (-\infty,0), \\
&\sigma_i \in (0,\infty), \\
&\gamma_i \in (0,1).
\end{eqnarray}

\section{General multi-period model}
Consider now a sequence of periods,
\begin{eqnarray}
[T^0,T^1),[T^1,T^2),\ldots, [T^{p_0-1},T^{p_0}),
\end{eqnarray}
and suppose that we also have 
a sequence of messaging records
\begin{eqnarray}
\mathcal M^1,\mathcal M^2,\ldots, \mathcal M^{p_0}.
\end{eqnarray}
For each $p \in \{1,\ldots, p_0\}$, we assume that  
the time-stamp of each message in $\mathcal M_p$ is from the interval 
$[T^{p-1},T^{p})$.  For each $p$, suppose that we have parameter $\Psi^p$ and 
assume that we have the partition $(C_1^p,C_2^p)$. Now, we now assume
that $\Psi^{p+1}$ is the maximizer of the full log-likelihood funtion 
$f_{[T^{p-1},T^{p})}(\mathcal M^p,Y^p;\Psi)$, where $Y^p$ is the Lamperati transform of $X$.  

Note that the MLE $\widehat{\Psi}^p$ of $\Psi^p$ need not be the same as $\Psi^{p+1}$ since $\widehat{\Psi}^p$ is the maximizer of the (marginal) log-likelihood $f_{[T^p,T^{p+1})}(\mathcal M^p;\Psi)$.  We now let 
$\Psi^p$ to be the parameter of $X$ for the period $[T^p,T^{p+1})$.  In particualr, $\widehat{\Psi}^{p+1}$ is the MLE of $\Psi^{p+1}$.  Also, note that $\Psi^{p+1}$ partitions the vertex set $[n]$ into $(C_1^{p+1},C_2^{p+1})$. 

To what end, should this be done?  
\begin{enumerate}
\item[(i)]  One must come up with a way to measure the error.
\item[(ii)] How do we incooporate the error measured into the new estimate? \& formalize the procedure.
\item[(iii)]  Experiment on some synthetic data \& validate the proposed procedure
\item[(iv)] Apply on the real data \& report on the findings.
\end{enumerate}
\end{document}
